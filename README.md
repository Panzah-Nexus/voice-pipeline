# ğŸ™ï¸ Local Voice Pipeline

**Fully local, air-gapped voice AI pipeline** using open-source components with no external dependencies.

## ğŸ—ï¸ Architecture

### **Cascaded Pipeline:**
```
Audio Input â†’ Whisper STT â†’ Ollama LLM â†’ Kokoro TTS â†’ Audio Output
```

### **Components:**
- **ğŸ™ï¸ STT**: WhisperSTTService (local, CUDA-accelerated)
- **ğŸ§  LLM**: OLLamaLLMService (local models with conversation memory) 
- **ğŸ—£ï¸ TTS**: KokoroTTSService (PyTorch-based, already working)
- **ğŸ”§ Framework**: Pipecat (handles audio, interruptions, real-time streaming)

## ğŸš€ Quick Start

### 1. Install Ollama
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a fast model for conversation
ollama pull llama3.2:3b

# Or for better quality (if you have enough GPU memory):
ollama pull llama3.1:8b
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Set Environment Variables
```bash
# Ollama Configuration
export OLLAMA_MODEL="llama3.2:3b"  # Or your preferred model
export OLLAMA_BASE_URL="http://localhost:11434/v1"

# Optional: CUDA for Whisper (auto-detected)
export CUDA_AVAILABLE="true"

# TTS Settings (existing)
export KOKORO_VOICE_ID="af_bella"
```

### 4. Run the Pipeline
```bash
python src/main.py
```

## âœ¨ Key Features

### **ğŸ”’ Fully Local & Private**
- âœ… No external API calls
- âœ… No data leaves your machine
- âœ… Complete conversation privacy

### **ğŸ’¬ Perfect Conversation Memory**
- âœ… OpenAI-compatible context management
- âœ… Maintains conversation history
- âœ… System prompt enforcement (English-only)

### **âš¡ Optimized Performance**
- âœ… CUDA acceleration for Whisper STT
- âœ… Fast interruption detection (150ms VAD)
- âœ… Streaming responses
- âœ… Minimal latency

### **ğŸ”§ Ready for Function Calling**
- âœ… Built-in tool/function support via OpenAI format
- âœ… Easy to add custom functions
- âœ… Extensible for future features

### **ğŸ› ï¸ Easy Debugging**
- âœ… Separate components = easier troubleshooting
- âœ… Individual component metrics
- âœ… Clear conversation flow logging

## ğŸ“Š Expected Performance

### **Latency Breakdown:**
- **STT (Whisper)**: ~200-400ms
- **LLM (Ollama 3B)**: ~300-800ms (depends on hardware)  
- **TTS (Kokoro)**: ~100-300ms
- **Total**: ~600ms-1.5s (much better than previous 1.8s+)

### **Interruption Response**: ~150ms (fast VAD)

## ğŸ¯ Model Recommendations

### **For Development/Testing:**
```bash
ollama pull llama3.2:1b    # Fastest (~200-400ms inference)
ollama pull llama3.2:3b    # Good balance (recommended)
```

### **For Production Quality:**
```bash
ollama pull llama3.1:8b    # Best quality, slower (~4.9GB)
ollama pull mistral:7b     # Alternative high-quality option
```

### **Whisper Models (auto-selected):**
- `DISTIL_MEDIUM_EN`: Fast English-only (~400MB, recommended)
- `MEDIUM`: Multilingual support (~769MB)  
- `LARGE`: Best accuracy (~1.5GB, slower)

## ğŸ”§ Configuration

### **System Prompt (Anti-Chinese Switching):**
The system prompt includes multiple safeguards against language switching:
```
1. ALWAYS respond in English only
2. If user speaks another language, understand but respond in English
3. Keep responses conversational and under 2 sentences
4. Remember conversation context
5. Immediately switch to English if non-English generation starts
```

### **Environment Variables:**
```bash
# Ollama LLM
OLLAMA_MODEL="llama3.2:3b"
OLLAMA_BASE_URL="http://localhost:11434/v1"

# Whisper STT  
CUDA_AVAILABLE="true"  # Enable CUDA acceleration

# Kokoro TTS (existing)
KOKORO_MODEL_PATH="/models/kokoro/model_fp16.onnx"
KOKORO_VOICES_PATH="/models/kokoro/voices-v1.0.bin"  
KOKORO_VOICE_ID="af_bella"
KOKORO_SAMPLE_RATE="24000"
```

## ğŸ†š Why Cascaded > Ultravox

| Feature | Ultravox (Pipecat) | Cascaded (New) |
|---------|--------------------|----|
| **Conversation Memory** | âŒ Broken/Limited | âœ… Full OpenAI Context |
| **Function Calling** | âŒ Not supported | âœ… Built-in support |
| **Debugging** | âŒ Black box | âœ… Component-level |
| **Performance Control** | âŒ Limited | âœ… Fine-tunable |
| **System Prompts** | âŒ Unreliable | âœ… Fully working |
| **Stability** | âŒ Experimental | âœ… Production-ready |

## ğŸ› ï¸ Future Extensions

This architecture makes it easy to add:
- **ğŸ”§ Function calling**: Weather, databases, APIs
- **ğŸ“Š Analytics**: Conversation insights
- **ğŸ¯ Custom models**: Local fine-tuned models
- **ğŸ”„ Model switching**: Dynamic model selection
- **ğŸ“± Multi-modal**: Vision, images (already supported by Ollama)

## ğŸ“ Deployment

Same Docker deployment process - just updated dependencies and models!

## ğŸ¯ Key Features

- **Ultravox**: Single model for both speech recognition and language understanding
- **Piper TTS**: Local neural text-to-speech (no APIs)
- **Air-Gapped**: All AI processing on GPU, no external API calls
- **Real-Time**: Low-latency voice conversations
- **Pipecat**: Production-ready voice pipeline framework

## ğŸ“ Project Structure

```
voice-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Entry point
â”‚   â”œâ”€â”€ pipecat_pipeline.py     # Main pipeline with Pipecat
â”‚   â””â”€â”€ piper_tts_service.py    # Custom Piper TTS service
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile              # GPU-enabled container
â”œâ”€â”€ local_client.py             # Audio capture/playback client
â”œâ”€â”€ cerebrium.toml              # Deployment configuration
â”œâ”€â”€ requirements.txt            # Server dependencies (GPU)
â””â”€â”€ local_client_requirements.txt  # Client dependencies (lightweight)
```

## ğŸ”§ Troubleshooting

### Common Issues

1. **WebSocket Connection Failed**
   - Check `WS_SERVER` environment variable
   - Verify deployment is running: `cerebrium list`

2. **No Audio Input/Output**
   - Check audio devices: `python -c "import sounddevice; print(sounddevice.query_devices())"`
   - Ensure microphone/speakers are connected

3. **GPU Memory Issues**
   - The 8B Ultravox model requires ~16GB VRAM
   - A10 has 24GB, should work fine

## ğŸ’° Cost

- **Cerebrium A10**: ~$0.50-1.00/hour when active
- **Auto-scaling**: Scales to zero when idle (no cost)
- **No API fees**: All models run locally on GPU

## ğŸ“š Documentation

See the `docs/` directory for detailed guides:
- [Architecture](docs/architecture.md)
- [Deployment Guide](docs/deployment_guide.md) 
- [API Setup](docs/api_setup.md)
- [Ultravox Setup](docs/ultravox_setup.md)
- [Piper Setup](docs/piper_setup.md)


