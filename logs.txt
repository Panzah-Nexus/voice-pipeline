2025-06-22T09:31:01.687699897Z ==========
2025-06-22T09:31:01.687732696Z == CUDA ==
2025-06-22T09:31:01.687861259Z ==========
2025-06-22T09:31:01.690629985Z CUDA Version 11.8.0
2025-06-22T09:31:01.691729776Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-06-22T09:31:01.692684789Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-06-22T09:31:01.692691018Z By pulling and using the container, you accept the terms and conditions of this license:
2025-06-22T09:31:01.692695946Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-06-22T09:31:01.692704959Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-06-22T09:31:01.716637943Z  * Starting OpenBSD Secure Shell server sshd
2025-06-22T09:31:01.725174035Z    ...done.
2025-06-22T09:31:01.725340685Z üîß Development mode starting...
2025-06-22T09:31:01.725412523Z üîÑ Cloning latest source code from cursor_oneshot branch...
2025-06-22T09:31:01.726925775Z Cloning into '/app/voice-pipeline-src'...
2025-06-22T09:31:04.573129536Z üìÅ Copying source code...
2025-06-22T09:31:04.576672184Z üöÄ Starting voice pipeline...
2025-06-22T09:31:04.886150039Z 2025-06-22 09:31:04.885 | INFO     | pipecat:<module>:14 - ·ìö·òè·ó¢ Pipecat 0.0.71 (Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]) ·ìö·òè·ó¢
2025-06-22T09:31:06.177350927Z 2025-06-22 09:31:06.177 | DEBUG    | pipecat.audio.vad.silero:__init__:111 - Loading Silero VAD model...
2025-06-22T09:31:06.219671530Z 2025-06-22 09:31:06.219 | DEBUG    | pipecat.audio.vad.silero:__init__:133 - Loaded Silero VAD
2025-06-22T09:31:13.640067749Z 2025-06-22 09:31:13.639 | ERROR    | pipecat.services.ultravox.stt:<module>:39 - Exception: No module named 'vllm'
2025-06-22T09:31:13.640114659Z 2025-06-22 09:31:13.639 | ERROR    | pipecat.services.ultravox.stt:<module>:40 - In order to use Ultravox, you need to `pip install pipecat-ai[ultravox]`.
2025-06-22T09:31:13.640543764Z Traceback (most recent call last):
2025-06-22T09:31:13.640568911Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/ultravox/stt.py", line 36, in <module>
2025-06-22T09:31:13.640575431Z     from vllm import AsyncLLMEngine, SamplingParams
2025-06-22T09:31:13.640580749Z ModuleNotFoundError: No module named 'vllm'
2025-06-22T09:31:13.640589853Z During handling of the above exception, another exception occurred:
2025-06-22T09:31:13.640598996Z Traceback (most recent call last):
2025-06-22T09:31:13.640603293Z   File "/app/src/main.py", line 31, in <module>
2025-06-22T09:31:13.640607509Z     from src.pipecat_pipeline import run_bot
2025-06-22T09:31:13.640611655Z   File "/app/src/pipecat_pipeline.py", line 51, in <module>
2025-06-22T09:31:13.640616112Z     from pipecat.services.ultravox.stt import UltravoxSTTService
2025-06-22T09:31:13.640620338Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/ultravox/__init__.py", line 11, in <module>
2025-06-22T09:31:13.640624575Z     from .stt import *
2025-06-22T09:31:13.640629652Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/ultravox/stt.py", line 41, in <module>
2025-06-22T09:31:13.640634009Z     raise Exception(f"Missing module: {e}")
2025-06-22T09:31:13.640638255Z Exception: Missing module: No module named 'vllm'
2025-06-22T09:31:18.583698662Z ==========
2025-06-22T09:31:18.583712943Z == CUDA ==
2025-06-22T09:31:18.583718552Z ==========
2025-06-22T09:31:18.587260849Z CUDA Version 11.8.0
2025-06-22T09:31:18.588170474Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-06-22T09:31:18.589076864Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-06-22T09:31:18.589082002Z By pulling and using the container, you accept the terms and conditions of this license:
2025-06-22T09:31:18.589086990Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-06-22T09:31:18.589115653Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-06-22T09:31:18.614494406Z  * Starting OpenBSD Secure Shell server sshd
2025-06-22T09:31:18.622326532Z    ...done.
2025-06-22T09:31:18.622542627Z üîß Development mode starting...
2025-06-22T09:31:18.622675937Z üîÑ Updating source code...
2025-06-22T09:31:19.278319125Z From https://github.com/Panzah-Nexus/voice-pipeline
2025-06-22T09:31:19.278354668Z  * branch            cascade_branch -> FETCH_HEAD
2025-06-22T09:31:19.616852652Z Updating 92c9cea..e82dfc4
2025-06-22T09:31:19.616884190Z Fast-forward
2025-06-22T09:31:19.624068073Z  MIGRATION_GUIDE.md       |  207 ++++++++
2025-06-22T09:31:19.624084557Z  README.md                |  172 +++++--
2025-06-22T09:31:19.624088523Z  cerebrium.toml           |   38 --
2025-06-22T09:31:19.624093391Z  docker/Dockerfile        |   98 ++--
2025-06-22T09:31:19.624096565Z  docker/Dockerfile.dev    |    6 +-
2025-06-22T09:31:19.624099570Z  logs_five_52.txt         | 1272 ----------------------------------------------
2025-06-22T09:31:19.624103145Z  requirements.txt         |   16 +-
2025-06-22T09:31:19.624106130Z  setup_local_pipeline.sh  |  162 ++++++
2025-06-22T09:31:19.624109144Z  src/README.md            |   50 +-
2025-06-22T09:31:19.624111989Z  src/llm_to_tts_bridge.py |   34 --
2025-06-22T09:31:19.624114913Z  src/main.py              |   17 +-
2025-06-22T09:31:19.624117857Z  src/pipecat_pipeline.py  |  194 +++----
2025-06-22T09:31:19.624120842Z  12 files changed, 728 insertions(+), 1538 deletions(-)
2025-06-22T09:31:19.624124167Z  create mode 100644 MIGRATION_GUIDE.md
2025-06-22T09:31:19.624127101Z  delete mode 100644 cerebrium.toml
2025-06-22T09:31:19.624130026Z  delete mode 100644 logs_five_52.txt
2025-06-22T09:31:19.624132900Z  create mode 100755 setup_local_pipeline.sh
2025-06-22T09:31:19.624135935Z  delete mode 100644 src/llm_to_tts_bridge.py
2025-06-22T09:31:19.625380913Z üìÅ Copying source code...
2025-06-22T09:31:19.627930781Z üöÄ Starting voice pipeline...
2025-06-22T09:31:19.942922472Z 2025-06-22 09:31:19.942 | INFO     | pipecat:<module>:14 - ·ìö·òè·ó¢ Pipecat 0.0.71 (Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]) ·ìö·òè·ó¢
2025-06-22T09:31:21.291502135Z 2025-06-22 09:31:21.291 | DEBUG    | pipecat.audio.vad.silero:__init__:111 - Loading Silero VAD model...
2025-06-22T09:31:21.333669949Z 2025-06-22 09:31:21.333 | DEBUG    | pipecat.audio.vad.silero:__init__:133 - Loaded Silero VAD
2025-06-22T09:31:34.479139653Z [32m09:31:34[0m | [36mpipecat_pipeline:91[0m | [1m    INFO[0m | [1müîß Initializing local cascaded pipeline components...[0m
2025-06-22T09:31:34.479171802Z 2025-06-22 09:31:34.478 | INFO     | src.pipecat_pipeline:<module>:91 - üîß Initializing local cascaded pipeline components...
2025-06-22T09:31:34.479179042Z [32m09:31:34[0m | [36mpipecat_pipeline:94[0m | [1m    INFO[0m | [1müìù Loading WhisperSTTService...[0m
2025-06-22T09:31:34.479183850Z 2025-06-22 09:31:34.478 | INFO     | src.pipecat_pipeline:<module>:94 - üìù Loading WhisperSTTService...
2025-06-22T09:31:34.577349065Z 2025-06-22 09:31:34.577 | DEBUG    | pipecat.services.whisper.stt:_load:340 - Loading Whisper model...
2025-06-22T09:31:42.757984831Z 2025-06-22 09:31:42.757 | DEBUG    | pipecat.services.whisper.stt:_load:344 - Loaded Whisper model
2025-06-22T09:31:42.758034435Z [32m09:31:42[0m | [36mpipecat_pipeline:100[0m | [1m    INFO[0m | [1m‚úÖ Whisper STT initialized successfully![0m
2025-06-22T09:31:42.758040825Z 2025-06-22 09:31:42.757 | INFO     | src.pipecat_pipeline:<module>:100 - ‚úÖ Whisper STT initialized successfully!
2025-06-22T09:31:42.758045412Z [32m09:31:42[0m | [36mpipecat_pipeline:103[0m | [1m    INFO[0m | [1müß† Loading Ollama LLM service...[0m
2025-06-22T09:31:42.758049738Z 2025-06-22 09:31:42.757 | INFO     | src.pipecat_pipeline:<module>:103 - üß† Loading Ollama LLM service...
2025-06-22T09:31:42.827268225Z [32m09:31:42[0m | [36mpipecat_pipeline:108[0m | [1m    INFO[0m | [1m‚úÖ Ollama LLM initialized successfully![0m
2025-06-22T09:31:42.827323288Z 2025-06-22 09:31:42.826 | INFO     | src.pipecat_pipeline:<module>:108 - ‚úÖ Ollama LLM initialized successfully!
2025-06-22T09:31:42.827329247Z [32m09:31:42[0m | [36mpipecat_pipeline:110[0m | [1m    INFO[0m | [1müéØ All local pipeline components ready![0m
2025-06-22T09:31:42.827334174Z 2025-06-22 09:31:42.827 | INFO     | src.pipecat_pipeline:<module>:110 - üéØ All local pipeline components ready!
2025-06-22T09:31:42.830040967Z 2025-06-22 09:31:42,829 - root - INFO - Starting voice pipeline server on 0.0.0.0:8000
2025-06-22T09:31:42.849555088Z INFO:     Started server process [73]
2025-06-22T09:31:42.849581007Z INFO:     Waiting for application startup.
2025-06-22T09:31:42.849852865Z INFO:     Application startup complete.
2025-06-22T09:31:42.850042069Z INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-06-22T09:32:01.521087042Z INFO:     100.64.0.34:48348 - "GET / HTTP/1.1" 200 OK
2025-06-22T09:32:04.718267549Z INFO:     100.64.0.31:36310 - "GET / HTTP/1.1" 200 OK
2025-06-22T09:32:05.396127114Z INFO:     100.64.0.31:36310 - "GET /favicon.ico HTTP/1.1" 404 Not Found
2025-06-22T09:33:08.076695388Z INFO:     100.64.0.32:60538 - "OPTIONS /connect HTTP/1.1" 200 OK
2025-06-22T09:33:08.324216947Z 2025-06-22 09:33:08,323 - root - INFO - Connect request - returning WebSocket URL: wss://x8ac4l0110x2ah-8000.proxy.runpod.net/ws
2025-06-22T09:33:08.324257388Z 2025-06-22 09:33:08,324 - root - INFO - Request headers: x-forwarded-host=x8ac4l0110x2ah-8000.proxy.runpod.net, x-forwarded-proto=https
2025-06-22T09:33:08.324805661Z INFO:     100.64.0.32:60538 - "POST /connect HTTP/1.1" 200 OK
2025-06-22T09:33:08.682252156Z INFO:     100.64.0.36:60694 - "WebSocket /ws" [accepted]
2025-06-22T09:33:08.682634370Z [32m09:33:08[0m | [36mpipecat_pipeline:115[0m | [1m    INFO[0m | [1müöÄ Starting local cascaded voice pipeline...[0m
2025-06-22T09:33:08.682648682Z 2025-06-22 09:33:08.682 | INFO     | src.pipecat_pipeline:run_bot:115 - üöÄ Starting local cascaded voice pipeline...
2025-06-22T09:33:08.683426499Z [32m09:33:08[0m | [36mkokoro_tts_service:130[0m | [1m    INFO[0m | [1mInitialising Kokoro TTS (model='/models/kokoro/model_fp16.onnx', voices='/models/kokoro/voices-v1.0.bin', voice='af_bella')[0m
2025-06-22T09:33:08.683488102Z 2025-06-22 09:33:08.683 | INFO     | src.kokoro_tts_service:__init__:130 - Initialising Kokoro TTS (model='/models/kokoro/model_fp16.onnx', voices='/models/kokoro/voices-v1.0.bin', voice='af_bella')
2025-06-22T09:33:09.088014534Z /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1
2025-06-22T09:33:09.088053833Z   warnings.warn(
2025-06-22T09:33:09.126129375Z /usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
2025-06-22T09:33:09.126166100Z   WeightNorm.apply(module, name, dim)
2025-06-22T09:33:14.641606406Z Collecting en-core-web-sm==3.8.0
2025-06-22T09:33:15.468983917Z   Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)
2025-06-22T09:33:15.826598205Z      ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 12.8/12.8 MB 52.1 MB/s eta 0:00:00
2025-06-22T09:33:16.181845370Z Installing collected packages: en-core-web-sm
2025-06-22T09:33:16.279034911Z Successfully installed en-core-web-sm-3.8.0
2025-06-22T09:33:16.279137635Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
2025-06-22T09:33:18.986244152Z [32m09:33:18[0m | [36mkokoro_tts_service:151[0m | [1m    INFO[0m | [1mKokoro pipeline initialised successfully (backend=PyTorch)[0m
2025-06-22T09:33:18.986320627Z 2025-06-22 09:33:18.985 | INFO     | src.kokoro_tts_service:__init__:151 - Kokoro pipeline initialised successfully (backend=PyTorch)
2025-06-22T09:33:18.986548980Z [32m09:33:18[0m | [36mkokoro_tts_service:155[0m | [1m    INFO[0m | [1müî• GPU acceleration enabled: NVIDIA L4[0m
2025-06-22T09:33:18.986562470Z 2025-06-22 09:33:18.986 | INFO     | src.kokoro_tts_service:__init__:155 - üî• GPU acceleration enabled: NVIDIA L4
2025-06-22T09:33:18.986612405Z [32m09:33:18[0m | [36mkokoro_tts_service:156[0m | [1m    INFO[0m | [1müî• GPU memory: 23.6GB[0m
2025-06-22T09:33:18.986621879Z 2025-06-22 09:33:18.986 | INFO     | src.kokoro_tts_service:__init__:156 - üî• GPU memory: 23.6GB
2025-06-22T09:33:18.987124464Z 2025-06-22 09:33:18.986 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking PipelineSource#0 -> FastAPIWebsocketInputTransport#0
2025-06-22T09:33:18.987150503Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking FastAPIWebsocketInputTransport#0 -> RTVIProcessor#0
2025-06-22T09:33:18.987229121Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking RTVIProcessor#0 -> WhisperSTTService#0
2025-06-22T09:33:18.987311875Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking WhisperSTTService#0 -> OpenAIUserContextAggregator#0
2025-06-22T09:33:18.987337694Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OpenAIUserContextAggregator#0 -> OLLamaLLMService#0
2025-06-22T09:33:18.987369141Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OLLamaLLMService#0 -> KokoroTTSService#0
2025-06-22T09:33:18.987415400Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking KokoroTTSService#0 -> FastAPIWebsocketOutputTransport#0
2025-06-22T09:33:18.987462972Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking FastAPIWebsocketOutputTransport#0 -> OpenAIAssistantContextAggregator#0
2025-06-22T09:33:18.987505145Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OpenAIAssistantContextAggregator#0 -> PipelineSink#0
2025-06-22T09:33:18.987765055Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking PipelineTaskSource#0 -> Pipeline#0
2025-06-22T09:33:18.987839537Z 2025-06-22 09:33:18.987 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking Pipeline#0 -> PipelineTaskSink#0
2025-06-22T09:33:18.987972807Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_interruption_start not registered[0m
2025-06-22T09:33:18.987985817Z 2025-06-22 09:33:18.987 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_interruption_start not registered
2025-06-22T09:33:18.988042662Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_interruption_end not registered[0m
2025-06-22T09:33:18.988049903Z 2025-06-22 09:33:18.987 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_interruption_end not registered
2025-06-22T09:33:18.988109392Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_user_started_speaking not registered[0m
2025-06-22T09:33:18.988115371Z 2025-06-22 09:33:18.988 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_user_started_speaking not registered
2025-06-22T09:33:18.988177304Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_user_stopped_speaking not registered[0m
2025-06-22T09:33:18.988184124Z 2025-06-22 09:33:18.988 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_user_stopped_speaking not registered
2025-06-22T09:33:18.988246207Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_bot_started_speaking not registered[0m
2025-06-22T09:33:18.988264665Z 2025-06-22 09:33:18.988 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_bot_started_speaking not registered
2025-06-22T09:33:18.988310804Z [32m09:33:18[0m | [36m     base_object:55[0m | [33m[1m WARNING[0m | [33m[1mEvent handler on_bot_stopped_speaking not registered[0m
2025-06-22T09:33:18.988318095Z 2025-06-22 09:33:18.988 | WARNING  | pipecat.utils.base_object:add_event_handler:55 - Event handler on_bot_stopped_speaking not registered
2025-06-22T09:33:18.988454450Z 2025-06-22 09:33:18.988 | DEBUG    | pipecat.pipeline.runner:run:38 - Runner PipelineRunner#0 started running PipelineTask#0
2025-06-22T09:33:18.989189083Z INFO:     connection open
2025-06-22T09:33:18.991920843Z 2025-06-22 09:33:18.991 | DEBUG    | pipecat.audio.vad.vad_analyzer:set_params:74 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-06-22T09:33:18.994914977Z [32m09:33:18[0m | [36mpipecat_pipeline:183[0m | [1m    INFO[0m | [1müîó Client connected to local pipeline[0m
2025-06-22T09:33:18.994923300Z 2025-06-22 09:33:18.994 | INFO     | src.pipecat_pipeline:on_client_connected:183 - üîó Client connected to local pipeline
2025-06-22T09:33:18.996306816Z 2025-06-22 09:33:18.996 | DEBUG    | pipecat.services.openai.base_llm:_stream_chat_completions:156 - OLLamaLLMService#0: Generating chat [[{"role": "system", "content": "You are a helpful AI assistant having a real-time voice conversation.\n\nCRITICAL INSTRUCTIONS:\n1. ALWAYS respond in English only - never Chinese, Japanese, Korean, or other languages\n2. If user speaks another language, understand it but respond in English\n3. Keep responses conversational and under 2 sentences for voice interaction\n4. Be natural, helpful, and engaging\n5. Remember our conversation context\n6. If you detect non-English generation starting, immediately switch to English\n\nYou are knowledgeable and can help with various topics while maintaining engaging conversation."}, {"role": "system", "content": "Please introduce yourself to the user."}]]
2025-06-22T09:33:19.017824885Z 2025-06-22 09:33:19,017 - openai._base_client - INFO - Retrying request to /chat/completions in 0.421704 seconds
2025-06-22T09:33:19.052805701Z 2025-06-22 09:33:19.052 | DEBUG    | pipecat.serializers.protobuf:deserialize:106 - ProtobufFrameSerializer: Transport message TransportMessageUrgentFrame#2(message: {'label': 'rtvi-ai', 'type': 'client-ready', 'data': {}, 'id': '99bd63b7'})
2025-06-22T09:33:19.053168275Z 2025-06-22 09:33:19.053 | DEBUG    | pipecat.processors.frameworks.rtvi:_handle_client_ready:844 - Received client-ready
2025-06-22T09:33:19.053250669Z [32m09:33:19[0m | [36mpipecat_pipeline:178[0m | [1m    INFO[0m | [1m‚úÖ Client ready - local cascaded pipeline active![0m
2025-06-22T09:33:19.053257559Z 2025-06-22 09:33:19.053 | INFO     | src.pipecat_pipeline:on_client_ready:178 - ‚úÖ Client ready - local cascaded pipeline active!
2025-06-22T09:33:19.448302661Z 2025-06-22 09:33:19,447 - openai._base_client - INFO - Retrying request to /chat/completions in 0.947714 seconds
2025-06-22T09:33:20.402591996Z 2025-06-22 09:33:20.402 | DEBUG    | pipecat.processors.metrics.frame_processor_metrics:stop_processing_metrics:84 - OLLamaLLMService#0 processing time: 1.4060509204864502
2025-06-22T09:33:20.431296271Z [32m09:33:20[0m | [36m         asyncio:113[0m | [31m[1m   ERROR[0m | [31m[1mOLLamaLLMService#0::__input_frame_task_handler: unexpected exception: Connection error.[0m
2025-06-22T09:33:20.431330232Z [33m[1mTraceback (most recent call last):[0m
2025-06-22T09:33:20.431343311Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
2025-06-22T09:33:20.431348930Z     yield
2025-06-22T09:33:20.431354268Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 394, in handle_async_request
2025-06-22T09:33:20.431359636Z     resp = await self._pool.handle_async_request(req)
2025-06-22T09:33:20.431364874Z                  ‚îÇ    ‚îÇ     ‚îÇ                    ‚îî <Request [b'POST']>
2025-06-22T09:33:20.431392175Z                  ‚îÇ    ‚îÇ     ‚îî <function AsyncConnectionPool.handle_async_request at 0x73a8f433e320>
2025-06-22T09:33:20.431397733Z                  ‚îÇ    ‚îî <AsyncConnectionPool [Requests: 0 active, 0 queued | Connections: 0 active, 0 idle]>
2025-06-22T09:33:20.431402911Z                  ‚îî <httpx.AsyncHTTPTransport object at 0x73a8ff132c80>
2025-06-22T09:33:20.431407828Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
2025-06-22T09:33:20.431412745Z     raise exc from None
2025-06-22T09:33:20.431418114Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
2025-06-22T09:33:20.431423041Z     response = await connection.handle_async_request(
2025-06-22T09:33:20.431427998Z                      ‚îÇ          ‚îî <function AsyncHTTPConnection.handle_async_request at 0x73a8f433d630>
2025-06-22T09:33:20.431432966Z                      ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.431437973Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 101, in handle_async_request
2025-06-22T09:33:20.431442971Z     raise exc
2025-06-22T09:33:20.431448028Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 78, in handle_async_request
2025-06-22T09:33:20.431453016Z     stream = await self._connect(request)
2025-06-22T09:33:20.431457913Z                    ‚îÇ    ‚îÇ        ‚îî <Request [b'POST']>
2025-06-22T09:33:20.431462791Z                    ‚îÇ    ‚îî <function AsyncHTTPConnection._connect at 0x73a8f433d6c0>
2025-06-22T09:33:20.431468269Z                    ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.431473286Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 124, in _connect
2025-06-22T09:33:20.431478314Z     stream = await self._network_backend.connect_tcp(**kwargs)
2025-06-22T09:33:20.431484844Z                    ‚îÇ    ‚îÇ                ‚îÇ             ‚îî {'host': 'localhost', 'port': 11434, 'local_address': None, 'timeout': 5.0, 'socket_options': None}
2025-06-22T09:33:20.431490082Z                    ‚îÇ    ‚îÇ                ‚îî <function AutoBackend.connect_tcp at 0x73a8f432b9a0>
2025-06-22T09:33:20.431495049Z                    ‚îÇ    ‚îî <httpcore._backends.auto.AutoBackend object at 0x73a8ff132530>
2025-06-22T09:33:20.431499796Z                    ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.431504643Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_backends/auto.py", line 31, in connect_tcp
2025-06-22T09:33:20.431509511Z     return await self._backend.connect_tcp(
2025-06-22T09:33:20.431514278Z                  ‚îÇ    ‚îÇ        ‚îî <function AnyIOBackend.connect_tcp at 0x73a8f4350ee0>
2025-06-22T09:33:20.431519255Z                  ‚îÇ    ‚îî <httpcore.AnyIOBackend object at 0x73a8f43ca050>
2025-06-22T09:33:20.431524473Z                  ‚îî <httpcore._backends.auto.AutoBackend object at 0x73a8ff132530>
2025-06-22T09:33:20.431529441Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py", line 113, in connect_tcp
2025-06-22T09:33:20.431534268Z     with map_exceptions(exc_map):
2025-06-22T09:33:20.431539305Z          ‚îÇ              ‚îî {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>, <class 'anyio...
2025-06-22T09:33:20.431546036Z          ‚îî <function map_exceptions at 0x73a8ff14b910>
2025-06-22T09:33:20.431551053Z   File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
2025-06-22T09:33:20.431555950Z     self.gen.throw(typ, value, traceback)
2025-06-22T09:33:20.431560848Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îÇ      ‚îî <traceback object at 0x73a8cf247f00>
2025-06-22T09:33:20.431565635Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îî OSError('All connection attempts failed')
2025-06-22T09:33:20.431570472Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îî <class 'OSError'>
2025-06-22T09:33:20.431575289Z     ‚îÇ    ‚îÇ   ‚îî <method 'throw' of 'generator' objects>
2025-06-22T09:33:20.431580167Z     ‚îÇ    ‚îî <generator object map_exceptions at 0x73a8cf24c3c0>
2025-06-22T09:33:20.431593587Z     ‚îî <contextlib._GeneratorContextManager object at 0x73a8cf248400>
2025-06-22T09:33:20.431598825Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py", line 14, in map_exceptions
2025-06-22T09:33:20.431603812Z     raise to_exc(exc) from exc
2025-06-22T09:33:20.431609090Z           ‚îî <class 'httpcore.ConnectError'>
2025-06-22T09:33:20.431618655Z [31m[1mhttpcore.ConnectError[0m:[1m All connection attempts failed[0m
2025-06-22T09:33:20.431633026Z [1mThe above exception was the direct cause of the following exception:[0m
2025-06-22T09:33:20.431650152Z [33m[1mTraceback (most recent call last):[0m
2025-06-22T09:33:20.431660117Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1500, in _request
2025-06-22T09:33:20.431665114Z     response = await self._client.send(
2025-06-22T09:33:20.431669992Z                      ‚îÇ    ‚îÇ       ‚îî <function AsyncClient.send at 0x73aad8979ea0>
2025-06-22T09:33:20.431674869Z                      ‚îÇ    ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.431680698Z                      ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.431685485Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1629, in send
2025-06-22T09:33:20.431690412Z     response = await self._send_handling_auth(
2025-06-22T09:33:20.431695260Z                      ‚îÇ    ‚îî <function AsyncClient._send_handling_auth at 0x73aad8979f30>
2025-06-22T09:33:20.431700147Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.431704974Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1657, in _send_handling_auth
2025-06-22T09:33:20.431709902Z     response = await self._send_handling_redirects(
2025-06-22T09:33:20.431714799Z                      ‚îÇ    ‚îî <function AsyncClient._send_handling_redirects at 0x73aad8979fc0>
2025-06-22T09:33:20.431719596Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.431724554Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1694, in _send_handling_redirects
2025-06-22T09:33:20.431729491Z     response = await self._send_single_request(request)
2025-06-22T09:33:20.431734378Z                      ‚îÇ    ‚îÇ                    ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.431739185Z                      ‚îÇ    ‚îî <function AsyncClient._send_single_request at 0x73aad897a050>
2025-06-22T09:33:20.431743973Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.431748790Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1730, in _send_single_request
2025-06-22T09:33:20.431753697Z     response = await transport.handle_async_request(request)
2025-06-22T09:33:20.431758504Z                      ‚îÇ         ‚îÇ                    ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.431763422Z                      ‚îÇ         ‚îî <function AsyncHTTPTransport.handle_async_request at 0x73aad895b130>
2025-06-22T09:33:20.431768259Z                      ‚îî <httpx.AsyncHTTPTransport object at 0x73a8ff132c80>
2025-06-22T09:33:20.431773637Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 393, in handle_async_request
2025-06-22T09:33:20.431778675Z     with map_httpcore_exceptions():
2025-06-22T09:33:20.431783572Z          ‚îî <function map_httpcore_exceptions at 0x73aad895a830>
2025-06-22T09:33:20.431788510Z   File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
2025-06-22T09:33:20.431793367Z     self.gen.throw(typ, value, traceback)
2025-06-22T09:33:20.431798254Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îÇ      ‚îî <traceback object at 0x73a8cf250600>
2025-06-22T09:33:20.431803141Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îî ConnectError(OSError('All connection attempts failed'))
2025-06-22T09:33:20.431815851Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îî <class 'httpcore.ConnectError'>
2025-06-22T09:33:20.431820998Z     ‚îÇ    ‚îÇ   ‚îî <method 'throw' of 'generator' objects>
2025-06-22T09:33:20.431826106Z     ‚îÇ    ‚îî <generator object map_httpcore_exceptions at 0x73a8cf21de00>
2025-06-22T09:33:20.431830923Z     ‚îî <contextlib._GeneratorContextManager object at 0x73a8cf248190>
2025-06-22T09:33:20.431835660Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
2025-06-22T09:33:20.431840387Z     raise mapped_exc(message) from exc
2025-06-22T09:33:20.431845255Z           ‚îÇ          ‚îî 'All connection attempts failed'
2025-06-22T09:33:20.431850022Z           ‚îî <class 'httpx.ConnectError'>
2025-06-22T09:33:20.431859486Z [31m[1mhttpx.ConnectError[0m:[1m All connection attempts failed[0m
2025-06-22T09:33:20.431873647Z [1mThe above exception was the direct cause of the following exception:[0m
2025-06-22T09:33:20.431887819Z [33m[1mTraceback (most recent call last):[0m
2025-06-22T09:33:20.431897293Z   File "[32m/app/src/[0m[32m[1mmain.py[0m", line [33m125[0m, in [35m<module>[0m
2025-06-22T09:33:20.431902330Z     [1masyncio[0m[35m[1m.[0m[1mrun[0m[1m([0m[1mmain[0m[1m([0m[1m)[0m[1m)[0m
2025-06-22T09:33:20.431907148Z     [36m‚îÇ       ‚îÇ   ‚îî [0m[36m[1m<function main at 0x73a8f4353910>[0m
2025-06-22T09:33:20.431911845Z     [36m‚îÇ       ‚îî [0m[36m[1m<function run at 0x73ab20a675b0>[0m
2025-06-22T09:33:20.431916642Z     [36m‚îî [0m[36m[1m<module 'asyncio' from '/usr/lib/python3.10/asyncio/__init__.py'>[0m
2025-06-22T09:33:20.431926036Z   File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
2025-06-22T09:33:20.431931003Z     return loop.run_until_complete(main)
2025-06-22T09:33:20.431935881Z            ‚îÇ    ‚îÇ                  ‚îî <coroutine object main at 0x73a8ff1d3a70>
2025-06-22T09:33:20.431940638Z            ‚îÇ    ‚îî <function BaseEventLoop.run_until_complete at 0x73ab200605e0>
2025-06-22T09:33:20.431945355Z            ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.431987098Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
2025-06-22T09:33:20.431997163Z     self.run_forever()
2025-06-22T09:33:20.432002340Z     ‚îÇ    ‚îî <function BaseEventLoop.run_forever at 0x73ab20060550>
2025-06-22T09:33:20.432007158Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.432012115Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
2025-06-22T09:33:20.432017273Z     self._run_once()
2025-06-22T09:33:20.432022190Z     ‚îÇ    ‚îî <function BaseEventLoop._run_once at 0x73ab20062050>
2025-06-22T09:33:20.432027018Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.432031825Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
2025-06-22T09:33:20.432036672Z     handle._run()
2025-06-22T09:33:20.432041569Z     ‚îÇ      ‚îî <function Handle._run at 0x73ab201b9a20>
2025-06-22T09:33:20.432046347Z     ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.432051174Z   File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
2025-06-22T09:33:20.432056151Z     self._context.run(self._callback, *self._args)
2025-06-22T09:33:20.432060999Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îÇ    ‚îî <member '_args' of 'Handle' objects>
2025-06-22T09:33:20.432065936Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.432070743Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îî <member '_callback' of 'Handle' objects>
2025-06-22T09:33:20.432075590Z     ‚îÇ    ‚îÇ            ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.432080448Z     ‚îÇ    ‚îî <member '_context' of 'Handle' objects>
2025-06-22T09:33:20.432085285Z     ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.432098295Z > File "/usr/local/lib/python3.10/dist-packages/pipecat/utils/asyncio.py", line 107, in run_coroutine
2025-06-22T09:33:20.432103512Z     await coroutine
2025-06-22T09:33:20.432108350Z           ‚îî <coroutine object FrameProcessor.__input_frame_task_handler at 0x73a8cffcb760>
2025-06-22T09:33:20.432114759Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/processors/frame_processor.py", line 378, in __input_frame_task_handler
2025-06-22T09:33:20.432119576Z     await self.process_frame(frame, direction)
2025-06-22T09:33:20.432124374Z           ‚îÇ    ‚îÇ             ‚îÇ      ‚îî <FrameDirection.DOWNSTREAM: 1>
2025-06-22T09:33:20.432129301Z           ‚îÇ    ‚îÇ             ‚îî OpenAILLMContextFrame(id=22, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destinat...
2025-06-22T09:33:20.432134268Z           ‚îÇ    ‚îî <function BaseOpenAILLMService.process_frame at 0x73aaae99cd30>
2025-06-22T09:33:20.432139146Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.432144694Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 298, in process_frame
2025-06-22T09:33:20.432149612Z     await self._process_context(context)
2025-06-22T09:33:20.432154519Z           ‚îÇ    ‚îÇ                ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x73a8e25a94b0>
2025-06-22T09:33:20.432159827Z           ‚îÇ    ‚îî <function BaseOpenAILLMService._process_context at 0x73aaae99cca0>
2025-06-22T09:33:20.432164684Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.432169531Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 191, in _process_context
2025-06-22T09:33:20.432174459Z     chunk_stream: AsyncStream[ChatCompletionChunk] = await self._stream_chat_completions(
2025-06-22T09:33:20.432179627Z                   ‚îÇ           ‚îÇ                            ‚îÇ    ‚îî <function BaseOpenAILLMService._stream_chat_completions at 0x73aaae99ca60>
2025-06-22T09:33:20.432184484Z                   ‚îÇ           ‚îÇ                            ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.432189461Z                   ‚îÇ           ‚îî <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>
2025-06-22T09:33:20.432194289Z                   ‚îî <class 'openai.AsyncStream'>
2025-06-22T09:33:20.432199046Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 175, in _stream_chat_completions
2025-06-22T09:33:20.432203993Z     chunks = await self.get_chat_completions(context, messages)
2025-06-22T09:33:20.432208921Z                    ‚îÇ    ‚îÇ                    ‚îÇ        ‚îî [{'role': 'system', 'content': 'You are a helpful AI assistant having a real-time voice conversation.\n\nCRITICAL INSTRUCTION...
2025-06-22T09:33:20.432214158Z                    ‚îÇ    ‚îÇ                    ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x73a8e25a94b0>
2025-06-22T09:33:20.432219096Z                    ‚îÇ    ‚îî <function BaseOpenAILLMService.get_chat_completions at 0x73aaae99caf0>
2025-06-22T09:33:20.432223903Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.432228660Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 150, in get_chat_completions
2025-06-22T09:33:20.432233477Z     chunks = await self._client.chat.completions.create(**params)
2025-06-22T09:33:20.432238635Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îÇ        ‚îî {'model': 'llama3.2:3b', 'stream': True, 'messages': [{'role': 'system', 'content': 'You are a helpful AI assistant having a ...
2025-06-22T09:33:20.432244033Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îî <function AsyncCompletions.create at 0x73aabeffb9a0>
2025-06-22T09:33:20.432248911Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x73a8e1a3f6d0>
2025-06-22T09:33:20.432261429Z                    ‚îÇ    ‚îÇ       ‚îî <openai.resources.chat.chat.AsyncChat object at 0x73a8ff132c20>
2025-06-22T09:33:20.432266717Z                    ‚îÇ    ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432271735Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.432276652Z   File "/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions/completions.py", line 2000, in create
2025-06-22T09:33:20.432281479Z     return await self._post(
2025-06-22T09:33:20.432286367Z                  ‚îÇ    ‚îî <bound method AsyncAPIClient.post of <openai.AsyncOpenAI object at 0x73a8ff132680>>
2025-06-22T09:33:20.432291424Z                  ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x73a8e1a3f6d0>
2025-06-22T09:33:20.432296252Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1767, in post
2025-06-22T09:33:20.432301189Z     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
2025-06-22T09:33:20.432306056Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îÇ                  ‚îî openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
2025-06-22T09:33:20.432311014Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îî True
2025-06-22T09:33:20.432315941Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îî FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_...
2025-06-22T09:33:20.432320778Z                  ‚îÇ    ‚îÇ       ‚îî <class 'openai.types.chat.chat_completion.ChatCompletion'>
2025-06-22T09:33:20.432325626Z                  ‚îÇ    ‚îî <function AsyncAPIClient.request at 0x73aac8d28670>
2025-06-22T09:33:20.432330523Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432335340Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1461, in request
2025-06-22T09:33:20.432340278Z     return await self._request(
2025-06-22T09:33:20.432346056Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.432351074Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432356131Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1524, in _request
2025-06-22T09:33:20.432361059Z     return await self._retry_request(
2025-06-22T09:33:20.432365926Z                  ‚îÇ    ‚îî <function AsyncAPIClient._retry_request at 0x73aac8d28790>
2025-06-22T09:33:20.432370793Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432375631Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1594, in _retry_request
2025-06-22T09:33:20.432380458Z     return await self._request(
2025-06-22T09:33:20.432385345Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.432390163Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432394980Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1524, in _request
2025-06-22T09:33:20.432401079Z     return await self._retry_request(
2025-06-22T09:33:20.432405796Z                  ‚îÇ    ‚îî <function AsyncAPIClient._retry_request at 0x73aac8d28790>
2025-06-22T09:33:20.432410563Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432415511Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1594, in _retry_request
2025-06-22T09:33:20.432420278Z     return await self._request(
2025-06-22T09:33:20.432425095Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.432430022Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.432434770Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1534, in _request
2025-06-22T09:33:20.432439587Z     raise APIConnectionError(request=request) from err
2025-06-22T09:33:20.432451895Z           ‚îÇ                          ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.432457193Z           ‚îî <class 'openai.APIConnectionError'>
2025-06-22T09:33:20.432466808Z [31m[1mopenai.APIConnectionError[0m:[1m Connection error.[0m
2025-06-22T09:33:20.441359786Z 2025-06-22 09:33:20.402 | ERROR    | pipecat.utils.asyncio:run_coroutine:113 - OLLamaLLMService#0::__input_frame_task_handler: unexpected exception: Connection error.
2025-06-22T09:33:20.441401729Z Traceback (most recent call last):
2025-06-22T09:33:20.441414067Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
2025-06-22T09:33:20.441423141Z     yield
2025-06-22T09:33:20.441427988Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 394, in handle_async_request
2025-06-22T09:33:20.441432936Z     resp = await self._pool.handle_async_request(req)
2025-06-22T09:33:20.441438524Z                  ‚îÇ    ‚îÇ     ‚îÇ                    ‚îî <Request [b'POST']>
2025-06-22T09:33:20.441443612Z                  ‚îÇ    ‚îÇ     ‚îî <function AsyncConnectionPool.handle_async_request at 0x73a8f433e320>
2025-06-22T09:33:20.441447938Z                  ‚îÇ    ‚îî <AsyncConnectionPool [Requests: 0 active, 0 queued | Connections: 0 active, 0 idle]>
2025-06-22T09:33:20.441452325Z                  ‚îî <httpx.AsyncHTTPTransport object at 0x73a8ff132c80>
2025-06-22T09:33:20.441456832Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
2025-06-22T09:33:20.441461178Z     raise exc from None
2025-06-22T09:33:20.441465725Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
2025-06-22T09:33:20.441469991Z     response = await connection.handle_async_request(
2025-06-22T09:33:20.441474338Z                      ‚îÇ          ‚îî <function AsyncHTTPConnection.handle_async_request at 0x73a8f433d630>
2025-06-22T09:33:20.441478775Z                      ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.441483131Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 101, in handle_async_request
2025-06-22T09:33:20.441487768Z     raise exc
2025-06-22T09:33:20.441492044Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 78, in handle_async_request
2025-06-22T09:33:20.441496411Z     stream = await self._connect(request)
2025-06-22T09:33:20.441500727Z                    ‚îÇ    ‚îÇ        ‚îî <Request [b'POST']>
2025-06-22T09:33:20.441505024Z                    ‚îÇ    ‚îî <function AsyncHTTPConnection._connect at 0x73a8f433d6c0>
2025-06-22T09:33:20.441509581Z                    ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.441513957Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py", line 124, in _connect
2025-06-22T09:33:20.441518304Z     stream = await self._network_backend.connect_tcp(**kwargs)
2025-06-22T09:33:20.441524203Z                    ‚îÇ    ‚îÇ                ‚îÇ             ‚îî {'host': 'localhost', 'port': 11434, 'local_address': None, 'timeout': 5.0, 'socket_options': None}
2025-06-22T09:33:20.441528850Z                    ‚îÇ    ‚îÇ                ‚îî <function AutoBackend.connect_tcp at 0x73a8f432b9a0>
2025-06-22T09:33:20.441533176Z                    ‚îÇ    ‚îî <httpcore._backends.auto.AutoBackend object at 0x73a8ff132530>
2025-06-22T09:33:20.441537483Z                    ‚îî <AsyncHTTPConnection [CONNECTION FAILED]>
2025-06-22T09:33:20.441541739Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_backends/auto.py", line 31, in connect_tcp
2025-06-22T09:33:20.441546146Z     return await self._backend.connect_tcp(
2025-06-22T09:33:20.441550472Z                  ‚îÇ    ‚îÇ        ‚îî <function AnyIOBackend.connect_tcp at 0x73a8f4350ee0>
2025-06-22T09:33:20.441554608Z                  ‚îÇ    ‚îî <httpcore.AnyIOBackend object at 0x73a8f43ca050>
2025-06-22T09:33:20.441558925Z                  ‚îî <httpcore._backends.auto.AutoBackend object at 0x73a8ff132530>
2025-06-22T09:33:20.441581288Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py", line 113, in connect_tcp
2025-06-22T09:33:20.441586126Z     with map_exceptions(exc_map):
2025-06-22T09:33:20.441590562Z          ‚îÇ              ‚îî {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>, <class 'anyio...
2025-06-22T09:33:20.441595640Z          ‚îî <function map_exceptions at 0x73a8ff14b910>
2025-06-22T09:33:20.441600107Z   File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
2025-06-22T09:33:20.441604724Z     self.gen.throw(typ, value, traceback)
2025-06-22T09:33:20.441609250Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îÇ      ‚îî <traceback object at 0x73a8cf247f00>
2025-06-22T09:33:20.441613447Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îî OSError('All connection attempts failed')
2025-06-22T09:33:20.441617703Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îî <class 'OSError'>
2025-06-22T09:33:20.441622060Z     ‚îÇ    ‚îÇ   ‚îî <method 'throw' of 'generator' objects>
2025-06-22T09:33:20.441626336Z     ‚îÇ    ‚îî <generator object map_exceptions at 0x73a8cf24c3c0>
2025-06-22T09:33:20.441630843Z     ‚îî <contextlib._GeneratorContextManager object at 0x73a8cf248400>
2025-06-22T09:33:20.441635780Z   File "/usr/local/lib/python3.10/dist-packages/httpcore/_exceptions.py", line 14, in map_exceptions
2025-06-22T09:33:20.441640217Z     raise to_exc(exc) from exc
2025-06-22T09:33:20.441645254Z           ‚îî <class 'httpcore.ConnectError'>
2025-06-22T09:33:20.441653627Z httpcore.ConnectError: All connection attempts failed
2025-06-22T09:33:20.441666046Z The above exception was the direct cause of the following exception:
2025-06-22T09:33:20.441678645Z Traceback (most recent call last):
2025-06-22T09:33:20.441687358Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1500, in _request
2025-06-22T09:33:20.441691824Z     response = await self._client.send(
2025-06-22T09:33:20.441696151Z                      ‚îÇ    ‚îÇ       ‚îî <function AsyncClient.send at 0x73aad8979ea0>
2025-06-22T09:33:20.441700377Z                      ‚îÇ    ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.441704734Z                      ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.441709050Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1629, in send
2025-06-22T09:33:20.441713347Z     response = await self._send_handling_auth(
2025-06-22T09:33:20.441717663Z                      ‚îÇ    ‚îî <function AsyncClient._send_handling_auth at 0x73aad8979f30>
2025-06-22T09:33:20.441721990Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.441726496Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1657, in _send_handling_auth
2025-06-22T09:33:20.441730793Z     response = await self._send_handling_redirects(
2025-06-22T09:33:20.441735149Z                      ‚îÇ    ‚îî <function AsyncClient._send_handling_redirects at 0x73aad8979fc0>
2025-06-22T09:33:20.441739416Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.441743702Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1694, in _send_handling_redirects
2025-06-22T09:33:20.441747959Z     response = await self._send_single_request(request)
2025-06-22T09:33:20.441752175Z                      ‚îÇ    ‚îÇ                    ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.441756451Z                      ‚îÇ    ‚îî <function AsyncClient._send_single_request at 0x73aad897a050>
2025-06-22T09:33:20.441760898Z                      ‚îî <openai._DefaultAsyncHttpxClient object at 0x73a8ff133940>
2025-06-22T09:33:20.441765154Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_client.py", line 1730, in _send_single_request
2025-06-22T09:33:20.441769511Z     response = await transport.handle_async_request(request)
2025-06-22T09:33:20.441773897Z                      ‚îÇ         ‚îÇ                    ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.441785345Z                      ‚îÇ         ‚îî <function AsyncHTTPTransport.handle_async_request at 0x73aad895b130>
2025-06-22T09:33:20.441789841Z                      ‚îî <httpx.AsyncHTTPTransport object at 0x73a8ff132c80>
2025-06-22T09:33:20.441794248Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 393, in handle_async_request
2025-06-22T09:33:20.441799035Z     with map_httpcore_exceptions():
2025-06-22T09:33:20.441803452Z          ‚îî <function map_httpcore_exceptions at 0x73aad895a830>
2025-06-22T09:33:20.441807728Z   File "/usr/lib/python3.10/contextlib.py", line 153, in __exit__
2025-06-22T09:33:20.441811945Z     self.gen.throw(typ, value, traceback)
2025-06-22T09:33:20.441816281Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îÇ      ‚îî <traceback object at 0x73a8cf250600>
2025-06-22T09:33:20.441820568Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îÇ    ‚îî ConnectError(OSError('All connection attempts failed'))
2025-06-22T09:33:20.441824864Z     ‚îÇ    ‚îÇ   ‚îÇ     ‚îî <class 'httpcore.ConnectError'>
2025-06-22T09:33:20.441829220Z     ‚îÇ    ‚îÇ   ‚îî <method 'throw' of 'generator' objects>
2025-06-22T09:33:20.441833477Z     ‚îÇ    ‚îî <generator object map_httpcore_exceptions at 0x73a8cf21de00>
2025-06-22T09:33:20.441837783Z     ‚îî <contextlib._GeneratorContextManager object at 0x73a8cf248190>
2025-06-22T09:33:20.441842020Z   File "/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
2025-06-22T09:33:20.441846376Z     raise mapped_exc(message) from exc
2025-06-22T09:33:20.441850693Z           ‚îÇ          ‚îî 'All connection attempts failed'
2025-06-22T09:33:20.441855039Z           ‚îî <class 'httpx.ConnectError'>
2025-06-22T09:33:20.441863512Z httpx.ConnectError: All connection attempts failed
2025-06-22T09:33:20.441876331Z The above exception was the direct cause of the following exception:
2025-06-22T09:33:20.441891464Z Traceback (most recent call last):
2025-06-22T09:33:20.441900287Z   File "/app/src/main.py", line 125, in <module>
2025-06-22T09:33:20.441904654Z     asyncio.run(main())
2025-06-22T09:33:20.441909221Z     ‚îÇ       ‚îÇ   ‚îî <function main at 0x73a8f4353910>
2025-06-22T09:33:20.441913787Z     ‚îÇ       ‚îî <function run at 0x73ab20a675b0>
2025-06-22T09:33:20.441918174Z     ‚îî <module 'asyncio' from '/usr/lib/python3.10/asyncio/__init__.py'>
2025-06-22T09:33:20.441926817Z   File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
2025-06-22T09:33:20.441931194Z     return loop.run_until_complete(main)
2025-06-22T09:33:20.441935550Z            ‚îÇ    ‚îÇ                  ‚îî <coroutine object main at 0x73a8ff1d3a70>
2025-06-22T09:33:20.441940037Z            ‚îÇ    ‚îî <function BaseEventLoop.run_until_complete at 0x73ab200605e0>
2025-06-22T09:33:20.441944514Z            ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.441963091Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
2025-06-22T09:33:20.441972025Z     self.run_forever()
2025-06-22T09:33:20.441976602Z     ‚îÇ    ‚îî <function BaseEventLoop.run_forever at 0x73ab20060550>
2025-06-22T09:33:20.441980958Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.441985145Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
2025-06-22T09:33:20.441989501Z     self._run_once()
2025-06-22T09:33:20.441994489Z     ‚îÇ    ‚îî <function BaseEventLoop._run_once at 0x73ab20062050>
2025-06-22T09:33:20.441998965Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-22T09:33:20.442003302Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
2025-06-22T09:33:20.442007548Z     handle._run()
2025-06-22T09:33:20.442011895Z     ‚îÇ      ‚îî <function Handle._run at 0x73ab201b9a20>
2025-06-22T09:33:20.442016241Z     ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.442028129Z   File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
2025-06-22T09:33:20.442032566Z     self._context.run(self._callback, *self._args)
2025-06-22T09:33:20.442036942Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îÇ    ‚îî <member '_args' of 'Handle' objects>
2025-06-22T09:33:20.442041319Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.442045645Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îî <member '_callback' of 'Handle' objects>
2025-06-22T09:33:20.442049852Z     ‚îÇ    ‚îÇ            ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.442054348Z     ‚îÇ    ‚îî <member '_context' of 'Handle' objects>
2025-06-22T09:33:20.442058815Z     ‚îî <Handle Task.task_wakeup(<Future finished result=None>)>
2025-06-22T09:33:20.442063041Z > File "/usr/local/lib/python3.10/dist-packages/pipecat/utils/asyncio.py", line 107, in run_coroutine
2025-06-22T09:33:20.442068900Z     await coroutine
2025-06-22T09:33:20.442073127Z           ‚îî <coroutine object FrameProcessor.__input_frame_task_handler at 0x73a8cffcb760>
2025-06-22T09:33:20.442078765Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/processors/frame_processor.py", line 378, in __input_frame_task_handler
2025-06-22T09:33:20.442083132Z     await self.process_frame(frame, direction)
2025-06-22T09:33:20.442087438Z           ‚îÇ    ‚îÇ             ‚îÇ      ‚îî <FrameDirection.DOWNSTREAM: 1>
2025-06-22T09:33:20.442091985Z           ‚îÇ    ‚îÇ             ‚îî OpenAILLMContextFrame(id=22, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destinat...
2025-06-22T09:33:20.442096472Z           ‚îÇ    ‚îî <function BaseOpenAILLMService.process_frame at 0x73aaae99cd30>
2025-06-22T09:33:20.442100718Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.442105185Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 298, in process_frame
2025-06-22T09:33:20.442109421Z     await self._process_context(context)
2025-06-22T09:33:20.442113667Z           ‚îÇ    ‚îÇ                ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x73a8e25a94b0>
2025-06-22T09:33:20.442119005Z           ‚îÇ    ‚îî <function BaseOpenAILLMService._process_context at 0x73aaae99cca0>
2025-06-22T09:33:20.442123662Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.442128319Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 191, in _process_context
2025-06-22T09:33:20.442132556Z     chunk_stream: AsyncStream[ChatCompletionChunk] = await self._stream_chat_completions(
2025-06-22T09:33:20.442136802Z                   ‚îÇ           ‚îÇ                            ‚îÇ    ‚îî <function BaseOpenAILLMService._stream_chat_completions at 0x73aaae99ca60>
2025-06-22T09:33:20.442141149Z                   ‚îÇ           ‚îÇ                            ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.442145595Z                   ‚îÇ           ‚îî <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>
2025-06-22T09:33:20.442149832Z                   ‚îî <class 'openai.AsyncStream'>
2025-06-22T09:33:20.442154118Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 175, in _stream_chat_completions
2025-06-22T09:33:20.442158334Z     chunks = await self.get_chat_completions(context, messages)
2025-06-22T09:33:20.442162681Z                    ‚îÇ    ‚îÇ                    ‚îÇ        ‚îî [{'role': 'system', 'content': 'You are a helpful AI assistant having a real-time voice conversation.\n\nCRITICAL INSTRUCTION...
2025-06-22T09:33:20.442167689Z                    ‚îÇ    ‚îÇ                    ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x73a8e25a94b0>
2025-06-22T09:33:20.442172125Z                    ‚îÇ    ‚îî <function BaseOpenAILLMService.get_chat_completions at 0x73aaae99caf0>
2025-06-22T09:33:20.442176372Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.442187568Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 150, in get_chat_completions
2025-06-22T09:33:20.442193027Z     chunks = await self._client.chat.completions.create(**params)
2025-06-22T09:33:20.442197363Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îÇ        ‚îî {'model': 'llama3.2:3b', 'stream': True, 'messages': [{'role': 'system', 'content': 'You are a helpful AI assistant having a ...
2025-06-22T09:33:20.442202401Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îî <function AsyncCompletions.create at 0x73aabeffb9a0>
2025-06-22T09:33:20.442207128Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x73a8e1a3f6d0>
2025-06-22T09:33:20.442211474Z                    ‚îÇ    ‚îÇ       ‚îî <openai.resources.chat.chat.AsyncChat object at 0x73a8ff132c20>
2025-06-22T09:33:20.442215981Z                    ‚îÇ    ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442220538Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x73a9ec7b0fa0>
2025-06-22T09:33:20.442224814Z   File "/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions/completions.py", line 2000, in create
2025-06-22T09:33:20.442229151Z     return await self._post(
2025-06-22T09:33:20.442233758Z                  ‚îÇ    ‚îî <bound method AsyncAPIClient.post of <openai.AsyncOpenAI object at 0x73a8ff132680>>
2025-06-22T09:33:20.442238885Z                  ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x73a8e1a3f6d0>
2025-06-22T09:33:20.442243983Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1767, in post
2025-06-22T09:33:20.442248269Z     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
2025-06-22T09:33:20.442252576Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îÇ                  ‚îî openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
2025-06-22T09:33:20.442256973Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îî True
2025-06-22T09:33:20.442261159Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îî FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_...
2025-06-22T09:33:20.442265425Z                  ‚îÇ    ‚îÇ       ‚îî <class 'openai.types.chat.chat_completion.ChatCompletion'>
2025-06-22T09:33:20.442269702Z                  ‚îÇ    ‚îî <function AsyncAPIClient.request at 0x73aac8d28670>
2025-06-22T09:33:20.442275290Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442279586Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1461, in request
2025-06-22T09:33:20.442283783Z     return await self._request(
2025-06-22T09:33:20.442288049Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.442292256Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442296502Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1524, in _request
2025-06-22T09:33:20.442300738Z     return await self._retry_request(
2025-06-22T09:33:20.442305555Z                  ‚îÇ    ‚îî <function AsyncAPIClient._retry_request at 0x73aac8d28790>
2025-06-22T09:33:20.442309932Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442314228Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1594, in _retry_request
2025-06-22T09:33:20.442318565Z     return await self._request(
2025-06-22T09:33:20.442322791Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.442327008Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442331274Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1524, in _request
2025-06-22T09:33:20.442335520Z     return await self._retry_request(
2025-06-22T09:33:20.442346667Z                  ‚îÇ    ‚îî <function AsyncAPIClient._retry_request at 0x73aac8d28790>
2025-06-22T09:33:20.442351364Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442355781Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1594, in _retry_request
2025-06-22T09:33:20.442361009Z     return await self._request(
2025-06-22T09:33:20.442365225Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x73aac8d28700>
2025-06-22T09:33:20.442369752Z                  ‚îî <openai.AsyncOpenAI object at 0x73a8ff132680>
2025-06-22T09:33:20.442373988Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1534, in _request
2025-06-22T09:33:20.442378255Z     raise APIConnectionError(request=request) from err
2025-06-22T09:33:20.442382451Z           ‚îÇ                          ‚îî <Request('POST', 'http://localhost:11434/v1/chat/completions')>
2025-06-22T09:33:20.442386807Z           ‚îî <class 'openai.APIConnectionError'>
2025-06-22T09:33:20.442395260Z openai.APIConnectionError: Connection error.
2025-06-22T09:33:26.990526012Z [32m09:33:26[0m | [36mpipecat_pipeline:227[0m | [1m    INFO[0m | [1müìä === LOCAL PIPELINE PERFORMANCE ===[0m
2025-06-22T09:33:26.990567724Z 2025-06-22 09:33:26.990 | INFO     | src.pipecat_pipeline:log_performance:227 - üìä === LOCAL PIPELINE PERFORMANCE ===
2025-06-22T09:33:26.990702647Z [32m09:33:26[0m | [36mpipecat_pipeline:230[0m | [1m    INFO[0m | [1müí¨ Conversation messages: 2[0m
2025-06-22T09:33:26.990728215Z 2025-06-22 09:33:26.990 | INFO     | src.pipecat_pipeline:log_performance:230 - üí¨ Conversation messages: 2
2025-06-22T09:33:26.990884019Z [32m09:33:26[0m | [36mpipecat_pipeline:243[0m | [1m    INFO[0m | [1müìä ================================[0m
2025-06-22T09:33:26.990907704Z 2025-06-22 09:33:26.990 | INFO     | src.pipecat_pipeline:log_performance:243 - üìä ================================
2025-06-22T09:33:34.992304653Z [32m09:33:34[0m | [36mpipecat_pipeline:227[0m | [1m    INFO[0m | [1müìä === LOCAL PIPELINE PERFORMANCE ===[0m
2025-06-22T09:33:34.992347157Z 2025-06-22 09:33:34.991 | INFO     | src.pipecat_pipeline:log_performance:227 - üìä === LOCAL PIPELINE PERFORMANCE ===
2025-06-22T09:33:34.992496972Z [32m09:33:34[0m | [36mpipecat_pipeline:230[0m | [1m    INFO[0m | [1müí¨ Conversation messages: 2[0m
2025-06-22T09:33:34.992522080Z 2025-06-22 09:33:34.992 | INFO     | src.pipecat_pipeline:log_performance:230 - üí¨ Conversation messages: 2
2025-06-22T09:33:34.992620638Z [32m09:33:34[0m | [36mpipecat_pipeline:243[0m | [1m    INFO[0m | [1müìä ================================[0m
2025-06-22T09:33:34.992637423Z 2025-06-22 09:33:34.992 | INFO     | src.pipecat_pipeline:log_performance:243 - üìä ================================
2025-06-22T09:33:36.388178245Z 2025-06-22 09:33:36.387 | DEBUG    | pipecat.transports.base_input:_handle_user_interruption:242 - User started speaking
2025-06-22T09:33:36.394938542Z 2025-06-22 09:33:36.394 | DEBUG    | pipecat.processors.metrics.frame_processor_metrics:stop_ttfb_metrics:69 - OLLamaLLMService#0 TTFB: 17.398478984832764
2025-06-22T09:33:36.401231768Z [32m09:33:36[0m | [36m        protobuf:64[0m | [33m[1m WARNING[0m | [33m[1mFrame type <class 'pipecat.frames.frames.StartInterruptionFrame'> is not serializable[0m
2025-06-22T09:33:36.401257637Z 2025-06-22 09:33:36.400 | WARNING  | pipecat.serializers.protobuf:serialize:64 - Frame type <class 'pipecat.frames.frames.StartInterruptionFrame'> is not serializable
2025-06-22T09:33:37.886780201Z 2025-06-22 09:33:37.886 | DEBUG    | pipecat.transports.base_input:_handle_user_interruption:265 - User stopped speaking
2025-06-22T09:33:37.889475917Z 2025-06-22 09:33:37,889 - faster_whisper - INFO - Processing audio with duration 00:02.505
2025-06-22T09:33:37.902274067Z Unable to load any of {libcudnn_cnn.so.9.1.0, libcudnn_cnn.so.9.1, libcudnn_cnn.so.9, libcudnn_cnn.so}
2025-06-22T09:33:37.902302009Z Invalid handle. Cannot load symbol cudnnCreateConvolutionDescriptor
2025-06-22T09:33:38.218128023Z /app/dev_start.sh: line 41:    73 Aborted                 (core dumped) python src/main.py
2025-06-22T09:33:43.982883311Z ==========
2025-06-22T09:33:43.982889580Z == CUDA ==
2025-06-22T09:33:43.982895589Z ==========
2025-06-22T09:33:43.986251947Z CUDA Version 11.8.0
2025-06-22T09:33:43.987179710Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-06-22T09:33:43.988073521Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-06-22T09:33:43.988076505Z By pulling and using the container, you accept the terms and conditions of this license:
2025-06-22T09:33:43.988078959Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-06-22T09:33:43.988082755Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-06-22T09:33:44.009335425Z  * Starting OpenBSD Secure Shell server sshd
2025-06-22T09:33:44.017228252Z    ...done.
2025-06-22T09:33:44.017467922Z üîß Development mode starting...
2025-06-22T09:33:44.017529444Z üîÑ Updating source code...
2025-06-22T09:33:44.707815802Z From https://github.com/Panzah-Nexus/voice-pipeline
2025-06-22T09:33:44.707854229Z  * branch            cascade_branch -> FETCH_HEAD
2025-06-22T09:33:44.718520169Z Already up to date.
2025-06-22T09:33:44.719189153Z üìÅ Copying source code...
2025-06-22T09:33:44.723079242Z üöÄ Starting voice pipeline...
2025-06-22T09:33:45.032885460Z 2025-06-22 09:33:45.032 | INFO     | pipecat:<module>:14 - ·ìö·òè·ó¢ Pipecat 0.0.71 (Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]) ·ìö·òè·ó¢
2025-06-22T09:33:46.364419728Z 2025-06-22 09:33:46.364 | DEBUG    | pipecat.audio.vad.silero:__init__:111 - Loading Silero VAD model...
2025-06-22T09:33:46.406202825Z 2025-06-22 09:33:46.405 | DEBUG    | pipecat.audio.vad.silero:__init__:133 - Loaded Silero VAD
2025-06-22T09:33:59.417380497Z [32m09:33:59[0m | [36mpipecat_pipeline:91[0m | [1m    INFO[0m | [1müîß Initializing local cascaded pipeline components...[0m
2025-06-22T09:33:59.417421388Z 2025-06-22 09:33:59.417 | INFO     | src.pipecat_pipeline:<module>:91 - üîß Initializing local cascaded pipeline components...
2025-06-22T09:33:59.417427107Z [32m09:33:59[0m | [36mpipecat_pipeline:94[0m | [1m    INFO[0m | [1müìù Loading WhisperSTTService...[0m
2025-06-22T09:33:59.417466025Z 2025-06-22 09:33:59.417 | INFO     | src.pipecat_pipeline:<module>:94 - üìù Loading WhisperSTTService...
2025-06-22T09:33:59.516395819Z 2025-06-22 09:33:59.516 | DEBUG    | pipecat.services.whisper.stt:_load:340 - Loading Whisper model...
2025-06-22T09:34:00.533427620Z 2025-06-22 09:34:00.533 | DEBUG    | pipecat.services.whisper.stt:_load:344 - Loaded Whisper model
2025-06-22T09:34:00.533541250Z [32m09:34:00[0m | [36mpipecat_pipeline:100[0m | [1m    INFO[0m | [1m‚úÖ Whisper STT initialized successfully![0m
2025-06-22T09:34:00.533568702Z 2025-06-22 09:34:00.533 | INFO     | src.pipecat_pipeline:<module>:100 - ‚úÖ Whisper STT initialized successfully!
2025-06-22T09:34:00.533574440Z [32m09:34:00[0m | [36mpipecat_pipeline:103[0m | [1m    INFO[0m | [1müß† Loading Ollama LLM service...[0m
2025-06-22T09:34:00.533579298Z 2025-06-22 09:34:00.533 | INFO     | src.pipecat_pipeline:<module>:103 - üß† Loading Ollama LLM service...
2025-06-22T09:34:00.602319496Z [32m09:34:00[0m | [36mpipecat_pipeline:108[0m | [1m    INFO[0m | [1m‚úÖ Ollama LLM initialized successfully![0m
2025-06-22T09:34:00.602366527Z 2025-06-22 09:34:00.602 | INFO     | src.pipecat_pipeline:<module>:108 - ‚úÖ Ollama LLM initialized successfully!
2025-06-22T09:34:00.602369722Z [32m09:34:00[0m | [36mpipecat_pipeline:110[0m | [1m    INFO[0m | [1müéØ All local pipeline components ready![0m
2025-06-22T09:34:00.602372316Z 2025-06-22 09:34:00.602 | INFO     | src.pipecat_pipeline:<module>:110 - üéØ All local pipeline components ready!
2025-06-22T09:34:00.605184857Z 2025-06-22 09:34:00,604 - root - INFO - Starting voice pipeline server on 0.0.0.0:8000
2025-06-22T09:34:00.625439479Z INFO:     Started server process [70]
2025-06-22T09:34:00.625461332Z INFO:     Waiting for application startup.
2025-06-22T09:34:00.625842374Z INFO:     Application startup complete.
2025-06-22T09:34:00.626104488Z INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)