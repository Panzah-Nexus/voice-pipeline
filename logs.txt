2025-06-23T20:47:49.363935459Z ==========
2025-06-23T20:47:49.363957060Z == CUDA ==
2025-06-23T20:47:49.364023927Z ==========
2025-06-23T20:47:49.367854393Z CUDA Version 11.8.0
2025-06-23T20:47:49.369264982Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-06-23T20:47:49.370465292Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-06-23T20:47:49.370468418Z By pulling and using the container, you accept the terms and conditions of this license:
2025-06-23T20:47:49.370470923Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-06-23T20:47:49.370475341Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-06-23T20:47:49.398956429Z  * Starting OpenBSD Secure Shell server sshd
2025-06-23T20:47:49.409386370Z    ...done.
2025-06-23T20:47:49.409672193Z üîß Development mode starting...
2025-06-23T20:47:49.409782923Z üîÑ Cloning latest source code from moonshine branch...
2025-06-23T20:47:49.411689214Z Cloning into '/app/voice-pipeline-src'...
2025-06-23T20:47:53.797209849Z üìÅ Copying source code...
2025-06-23T20:47:53.805626997Z üöÄ Starting voice pipeline...
2025-06-23T20:47:54.262274968Z 2025-06-23 20:47:54.262 | INFO     | pipecat:<module>:14 - ·ìö·òè·ó¢ Pipecat 0.0.71 (Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]) ·ìö·òè·ó¢
2025-06-23T20:47:58.922467113Z 2025-06-23 20:47:58.922 | DEBUG    | pipecat.audio.vad.silero:__init__:111 - Loading Silero VAD model...
2025-06-23T20:47:58.961943286Z 2025-06-23 20:47:58.961 | DEBUG    | pipecat.audio.vad.silero:__init__:133 - Loaded Silero VAD
2025-06-23T20:48:01.891633120Z 2025-06-23 20:48:01,891 - root - INFO - Starting voice pipeline server on 0.0.0.0:8000
2025-06-23T20:48:01.912501077Z INFO:     Started server process [102]
2025-06-23T20:48:01.912523199Z INFO:     Waiting for application startup.
2025-06-23T20:48:01.912935232Z INFO:     Application startup complete.
2025-06-23T20:48:01.913233558Z INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-06-23T20:59:46.065116017Z INFO:     100.64.0.32:42602 - "GET / HTTP/1.1" 200 OK
2025-06-23T20:59:47.562944810Z INFO:     100.64.0.32:42602 - "GET /favicon.ico HTTP/1.1" 404 Not Found
2025-06-23T21:00:28.767278811Z INFO:     100.64.0.32:47762 - "OPTIONS /connect HTTP/1.1" 200 OK
2025-06-23T21:00:28.844961454Z 2025-06-23 21:00:28,844 - root - INFO - Connect request - returning WebSocket URL: wss://ws7adv366nvfn2-8000.proxy.runpod.net/ws
2025-06-23T21:00:28.844983596Z 2025-06-23 21:00:28,844 - root - INFO - Request headers: x-forwarded-host=ws7adv366nvfn2-8000.proxy.runpod.net, x-forwarded-proto=https
2025-06-23T21:00:28.845400277Z INFO:     100.64.0.32:47762 - "POST /connect HTTP/1.1" 200 OK
2025-06-23T21:00:29.267336095Z INFO:     100.64.0.36:52870 - "WebSocket /ws" [accepted]
2025-06-23T21:00:29.267852867Z 2025-06-23 21:00:29.267 | INFO     | src.pipecat_pipeline:run_bot:54 - Starting bot
2025-06-23T21:00:29.268768696Z 2025-06-23 21:00:29.268 | DEBUG    | pipecat.audio.vad.silero:__init__:111 - Loading Silero VAD model...
2025-06-23T21:00:29.338208202Z 2025-06-23 21:00:29.337 | DEBUG    | pipecat.audio.vad.silero:__init__:133 - Loaded Silero VAD
2025-06-23T21:00:29.338578144Z 2025-06-23 21:00:29.338 | DEBUG    | src.moonshine.stt:_load:100 - Loading Moonshine ONNX model...
2025-06-23T21:00:34.162873121Z 2025-06-23 21:00:34.162 | DEBUG    | src.moonshine.stt:_load:102 - Loaded Moonshine model.
2025-06-23T21:00:34.165176506Z 2025-06-23 21:00:34.164 | INFO     | src.kokoro.tts:__init__:91 - Initializing Kokoro TTS service with model_path: /app/assets/kokoro-v1.0.onnx and voices_path: /app/assets/voices-v1.0.bin
2025-06-23T21:00:35.402130917Z 2025-06-23 21:00:35.401 | INFO     | src.kokoro.tts:__init__:93 - Kokoro initialized
2025-06-23T21:00:35.402183487Z 2025-06-23 21:00:35.402 | INFO     | src.kokoro.tts:__init__:102 - Kokoro TTS service initialized
2025-06-23T21:00:35.493657425Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking PipelineSource#0 -> FastAPIWebsocketInputTransport#0
2025-06-23T21:00:35.493706478Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking FastAPIWebsocketInputTransport#0 -> MoonshineSTTService#0
2025-06-23T21:00:35.493761222Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking MoonshineSTTService#0 -> OpenAIUserContextAggregator#0
2025-06-23T21:00:35.494088334Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OpenAIUserContextAggregator#0 -> OLLamaLLMService#0
2025-06-23T21:00:35.494092712Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OLLamaLLMService#0 -> KokoroTTSService#0
2025-06-23T21:00:35.494096529Z 2025-06-23 21:00:35.493 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking KokoroTTSService#0 -> FastAPIWebsocketOutputTransport#0
2025-06-23T21:00:35.494100747Z 2025-06-23 21:00:35.494 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking FastAPIWebsocketOutputTransport#0 -> OpenAIAssistantContextAggregator#0
2025-06-23T21:00:35.494182062Z 2025-06-23 21:00:35.494 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking OpenAIAssistantContextAggregator#0 -> PipelineSink#0
2025-06-23T21:00:35.495412920Z 2025-06-23 21:00:35.494 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking PipelineTaskSource#0 -> Pipeline#0
2025-06-23T21:00:35.496327477Z 2025-06-23 21:00:35.495 | DEBUG    | pipecat.processors.frame_processor:link:202 - Linking Pipeline#0 -> PipelineTaskSink#0
2025-06-23T21:00:35.496657954Z 2025-06-23 21:00:35.496 | DEBUG    | pipecat.pipeline.runner:run:38 - Runner PipelineRunner#0 started running PipelineTask#0
2025-06-23T21:00:35.497452293Z INFO:     connection open
2025-06-23T21:00:35.500322004Z 2025-06-23 21:00:35.499 | DEBUG    | pipecat.audio.vad.vad_analyzer:set_params:74 - Setting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6
2025-06-23T21:00:35.500780134Z 2025-06-23 21:00:35.500 | ERROR    | pipecat.processors.frame_processor:_check_started:351 - RTVIProcessor#0 Trying to process TransportMessageUrgentFrame#0(message: {'label': 'rtvi-ai', 'type': 'metrics', 'data': {'ttfb': [{'processor': 'OLLamaLLMService#0', 'value': 0.0}, {'processor': 'KokoroTTSService#0', 'value': 0.0}], 'processing': [{'processor': 'OLLamaLLMService#0', 'value': 0.0}, {'processor': 'KokoroTTSService#0', 'value': 0.0}]}}) but StartFrame not received yet
2025-06-23T21:00:35.502618456Z 2025-06-23 21:00:35.502 | DEBUG    | pipecat.services.openai.base_llm:_stream_chat_completions:156 - OLLamaLLMService#0: Generating chat [[{"role": "system", "content": "You are a helpful LLM. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way."}, {"role": "system", "content": "Please introduce yourself to the user."}]]
2025-06-23T21:00:35.516683541Z 2025-06-23 21:00:35.516 | ERROR    | pipecat.processors.frame_processor:_check_started:351 - RTVIProcessor#0 Trying to process TransportMessageUrgentFrame#1(message: {'label': 'rtvi-ai', 'type': 'bot-llm-started'}) but StartFrame not received yet
2025-06-23T21:00:35.529846403Z 2025-06-23 21:00:35,529 - httpx - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-06-23T21:00:35.531390396Z 2025-06-23 21:00:35.531 | DEBUG    | pipecat.processors.metrics.frame_processor_metrics:stop_processing_metrics:84 - OLLamaLLMService#0 processing time: 0.028888225555419922
2025-06-23T21:00:35.540316981Z 2025-06-23 21:00:35.531 | ERROR    | pipecat.utils.asyncio:run_coroutine:113 - OLLamaLLMService#0::__input_frame_task_handler: unexpected exception: Error code: 404 - {'error': {'message': 'model "llama3.1:8b" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}
2025-06-23T21:00:35.540329104Z Traceback (most recent call last):
2025-06-23T21:00:35.540345716Z   File "/app/src/main.py", line 116, in <module>
2025-06-23T21:00:35.540349403Z     asyncio.run(main())
2025-06-23T21:00:35.540353521Z     ‚îÇ       ‚îÇ   ‚îî <function main at 0x78982cabb9a0>
2025-06-23T21:00:35.540357488Z     ‚îÇ       ‚îî <function run at 0x789974d775b0>
2025-06-23T21:00:35.540361295Z     ‚îî <module 'asyncio' from '/usr/lib/python3.10/asyncio/__init__.py'>
2025-06-23T21:00:35.540368439Z   File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
2025-06-23T21:00:35.540371765Z     return loop.run_until_complete(main)
2025-06-23T21:00:35.540375643Z            ‚îÇ    ‚îÇ                  ‚îî <coroutine object main at 0x789973140ac0>
2025-06-23T21:00:35.540379270Z            ‚îÇ    ‚îî <function BaseEventLoop.run_until_complete at 0x7899743685e0>
2025-06-23T21:00:35.540385592Z            ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-23T21:00:35.540388938Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
2025-06-23T21:00:35.540392094Z     self.run_forever()
2025-06-23T21:00:35.540395140Z     ‚îÇ    ‚îî <function BaseEventLoop.run_forever at 0x789974368550>
2025-06-23T21:00:35.540398516Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-23T21:00:35.540401853Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
2025-06-23T21:00:35.540404818Z     self._run_once()
2025-06-23T21:00:35.540408355Z     ‚îÇ    ‚îî <function BaseEventLoop._run_once at 0x78997436a050>
2025-06-23T21:00:35.540411631Z     ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>
2025-06-23T21:00:35.540415418Z   File "/usr/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
2025-06-23T21:00:35.540418785Z     handle._run()
2025-06-23T21:00:35.540421891Z     ‚îÇ      ‚îî <function Handle._run at 0x7899744bda20>
2025-06-23T21:00:35.540424916Z     ‚îî <Handle <TaskStepMethWrapper object at 0x789827ac8340>()>
2025-06-23T21:00:35.540427842Z   File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
2025-06-23T21:00:35.540430908Z     self._context.run(self._callback, *self._args)
2025-06-23T21:00:35.540434014Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îÇ    ‚îî <member '_args' of 'Handle' objects>
2025-06-23T21:00:35.540437069Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îî <Handle <TaskStepMethWrapper object at 0x789827ac8340>()>
2025-06-23T21:00:35.540440847Z     ‚îÇ    ‚îÇ            ‚îÇ    ‚îî <member '_callback' of 'Handle' objects>
2025-06-23T21:00:35.540446968Z     ‚îÇ    ‚îÇ            ‚îî <Handle <TaskStepMethWrapper object at 0x789827ac8340>()>
2025-06-23T21:00:35.540450295Z     ‚îÇ    ‚îî <member '_context' of 'Handle' objects>
2025-06-23T21:00:35.540453270Z     ‚îî <Handle <TaskStepMethWrapper object at 0x789827ac8340>()>
2025-06-23T21:00:35.540456406Z > File "/usr/local/lib/python3.10/dist-packages/pipecat/utils/asyncio.py", line 107, in run_coroutine
2025-06-23T21:00:35.540459542Z     await coroutine
2025-06-23T21:00:35.540462578Z           ‚îî <coroutine object FrameProcessor.__input_frame_task_handler at 0x78981c11b680>
2025-06-23T21:00:35.540465714Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/processors/frame_processor.py", line 378, in __input_frame_task_handler
2025-06-23T21:00:35.540468940Z     await self.process_frame(frame, direction)
2025-06-23T21:00:35.540472146Z           ‚îÇ    ‚îÇ             ‚îÇ      ‚îî <FrameDirection.DOWNSTREAM: 1>
2025-06-23T21:00:35.540475232Z           ‚îÇ    ‚îÇ             ‚îî OpenAILLMContextFrame(id=18, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destinat...
2025-06-23T21:00:35.540478999Z           ‚îÇ    ‚îî <function BaseOpenAILLMService.process_frame at 0x789827cf6290>
2025-06-23T21:00:35.540482305Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x789827b2e650>
2025-06-23T21:00:35.540485401Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 298, in process_frame
2025-06-23T21:00:35.540488417Z     await self._process_context(context)
2025-06-23T21:00:35.540491423Z           ‚îÇ    ‚îÇ                ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x789827b2d3c0>
2025-06-23T21:00:35.540500941Z           ‚îÇ    ‚îî <function BaseOpenAILLMService._process_context at 0x789827cf6320>
2025-06-23T21:00:35.540505870Z           ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x789827b2e650>
2025-06-23T21:00:35.540508996Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 191, in _process_context
2025-06-23T21:00:35.540512122Z     chunk_stream: AsyncStream[ChatCompletionChunk] = await self._stream_chat_completions(
2025-06-23T21:00:35.540515158Z                   ‚îÇ           ‚îÇ                            ‚îÇ    ‚îî <function BaseOpenAILLMService._stream_chat_completions at 0x789827cf6050>
2025-06-23T21:00:35.540518304Z                   ‚îÇ           ‚îÇ                            ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x789827b2e650>
2025-06-23T21:00:35.540521710Z                   ‚îÇ           ‚îî <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>
2025-06-23T21:00:35.540524986Z                   ‚îî <class 'openai.AsyncStream'>
2025-06-23T21:00:35.540528824Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 175, in _stream_chat_completions
2025-06-23T21:00:35.540532140Z     chunks = await self.get_chat_completions(context, messages)
2025-06-23T21:00:35.540535957Z                    ‚îÇ    ‚îÇ                    ‚îÇ        ‚îî [{'role': 'system', 'content': "You are a helpful LLM. Your goal is to demonstrate your capabilities in a succinct way. Your ...
2025-06-23T21:00:35.540539564Z                    ‚îÇ    ‚îÇ                    ‚îî <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x789827b2d3c0>
2025-06-23T21:00:35.540542870Z                    ‚îÇ    ‚îî <function BaseOpenAILLMService.get_chat_completions at 0x789827cf5fc0>
2025-06-23T21:00:35.540546648Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x789827b2e650>
2025-06-23T21:00:35.540550425Z   File "/usr/local/lib/python3.10/dist-packages/pipecat/services/openai/base_llm.py", line 150, in get_chat_completions
2025-06-23T21:00:35.540553661Z     chunks = await self._client.chat.completions.create(**params)
2025-06-23T21:00:35.540556747Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îÇ        ‚îî {'model': 'llama3.1:8b', 'stream': True, 'messages': [{'role': 'system', 'content': "You are a helpful LLM. Your goal is to d...
2025-06-23T21:00:35.540560564Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îÇ           ‚îî <function AsyncCompletions.create at 0x78982ab79f30>
2025-06-23T21:00:35.540563710Z                    ‚îÇ    ‚îÇ       ‚îÇ    ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x7898279d0370>
2025-06-23T21:00:35.540566806Z                    ‚îÇ    ‚îÇ       ‚îî <openai.resources.chat.chat.AsyncChat object at 0x789827b2fe20>
2025-06-23T21:00:35.540570282Z                    ‚îÇ    ‚îî <openai.AsyncOpenAI object at 0x789827b2fc70>
2025-06-23T21:00:35.540573418Z                    ‚îî <pipecat.services.ollama.llm.OLLamaLLMService object at 0x789827b2e650>
2025-06-23T21:00:35.540576795Z   File "/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions/completions.py", line 2000, in create
2025-06-23T21:00:35.540580201Z     return await self._post(
2025-06-23T21:00:35.540583898Z                  ‚îÇ    ‚îî <bound method AsyncAPIClient.post of <openai.AsyncOpenAI object at 0x789827b2fc70>>
2025-06-23T21:00:35.540591172Z                  ‚îî <openai.resources.chat.completions.completions.AsyncCompletions object at 0x7898279d0370>
2025-06-23T21:00:35.540594518Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1767, in post
2025-06-23T21:00:35.540597674Z     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
2025-06-23T21:00:35.540600810Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îÇ                  ‚îî openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
2025-06-23T21:00:35.540604337Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îÇ            ‚îî True
2025-06-23T21:00:35.540613695Z                  ‚îÇ    ‚îÇ       ‚îÇ        ‚îî FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_...
2025-06-23T21:00:35.540616891Z                  ‚îÇ    ‚îÇ       ‚îî <class 'openai.types.chat.chat_completion.ChatCompletion'>
2025-06-23T21:00:35.540619937Z                  ‚îÇ    ‚îî <function AsyncAPIClient.request at 0x78982b04ab90>
2025-06-23T21:00:35.540623003Z                  ‚îî <openai.AsyncOpenAI object at 0x789827b2fc70>
2025-06-23T21:00:35.540626149Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1461, in request
2025-06-23T21:00:35.540629275Z     return await self._request(
2025-06-23T21:00:35.540632671Z                  ‚îÇ    ‚îî <function AsyncAPIClient._request at 0x78982b04ac20>
2025-06-23T21:00:35.540636128Z                  ‚îî <openai.AsyncOpenAI object at 0x789827b2fc70>
2025-06-23T21:00:35.540639183Z   File "/usr/local/lib/python3.10/dist-packages/openai/_base_client.py", line 1562, in _request
2025-06-23T21:00:35.540643091Z     raise self._make_status_error_from_response(err.response) from None
2025-06-23T21:00:35.540646066Z           ‚îÇ    ‚îî <function BaseClient._make_status_error_from_response at 0x78982b048dc0>
2025-06-23T21:00:35.540649082Z           ‚îî <openai.AsyncOpenAI object at 0x789827b2fc70>
2025-06-23T21:00:35.540655084Z openai.NotFoundError: Error code: 404 - {'error': {'message': 'model "llama3.1:8b" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}
2025-06-23T21:00:35.540662448Z 2025-06-23 21:00:35.540 | ERROR    | pipecat.processors.frame_processor:_check_started:351 - RTVIProcessor#0 Trying to process TransportMessageUrgentFrame#2(message: {'label': 'rtvi-ai', 'type': 'metrics', 'data': {'processing': [{'processor': 'OLLamaLLMService#0', 'model': 'llama3.1:8b', 'value': 0.028888225555419922}]}}) but StartFrame not received yet
2025-06-23T21:00:35.541211771Z 2025-06-23 21:00:35.541 | ERROR    | pipecat.processors.frame_processor:_check_started:351 - RTVIProcessor#0 Trying to process TransportMessageUrgentFrame#3(message: {'label': 'rtvi-ai', 'type': 'bot-llm-stopped'}) but StartFrame not received yet
2025-06-23T21:00:35.636436063Z 2025-06-23 21:00:35.636 | DEBUG    | pipecat.serializers.protobuf:deserialize:106 - ProtobufFrameSerializer: Transport message TransportMessageUrgentFrame#4(message: {'label': 'rtvi-ai', 'type': 'client-ready', 'data': {}, 'id': 'b1ee8fe3'})