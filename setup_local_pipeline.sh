#!/bin/bash

# üéôÔ∏è Local Voice Pipeline Setup Script
# ====================================
# Sets up the complete local cascaded pipeline: Whisper + Ollama + Kokoro

set -e  # Exit on any error

echo "üéôÔ∏è Setting up Local Voice Pipeline..."
echo "===================================="

# Check if running on Linux/macOS
if [[ "$OSTYPE" != "linux-gnu"* ]] && [[ "$OSTYPE" != "darwin"* ]]; then
    echo "‚ùå This script supports Linux and macOS only"
    exit 1
fi

# 1. Install Ollama if not already installed
echo "üîß Step 1: Installing Ollama..."
if ! command -v ollama &> /dev/null; then
    echo "   Installing Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "‚úÖ Ollama installed successfully"
else
    echo "‚úÖ Ollama already installed"
fi

# 2. Start Ollama service (if not running)
echo "üöÄ Step 2: Starting Ollama service..."
if ! pgrep -x "ollama" > /dev/null; then
    echo "   Starting Ollama daemon..."
    ollama serve &
    sleep 3  # Give it time to start
    echo "‚úÖ Ollama service started"
else
    echo "‚úÖ Ollama service already running"
fi

# 3. Download recommended model
echo "üß† Step 3: Downloading LLM model..."
MODEL_SIZE=${1:-"3b"}  # Default to 3b, allow override

case $MODEL_SIZE in
    "1b")
        MODEL="llama3.2:1b"
        echo "   Downloading fast model (1B params) - good for testing..."
        ;;
    "3b")
        MODEL="llama3.2:3b"
        echo "   Downloading balanced model (3B params) - recommended..."
        ;;
    "7b")
        MODEL="llama3.1:7b"
        echo "   Downloading high-quality model (7B params) - slower but better..."
        ;;
    *)
        echo "‚ùå Invalid model size. Use: 1b, 3b, or 7b"
        exit 1
        ;;
esac

ollama pull $MODEL
echo "‚úÖ Model $MODEL downloaded successfully"

# 4. Install Python dependencies
echo "üì¶ Step 4: Installing Python dependencies..."
if [ -f "requirements.txt" ]; then
    pip install -r requirements.txt
    echo "‚úÖ Python dependencies installed"
else
    echo "‚ö†Ô∏è  requirements.txt not found - please install manually"
fi

# 5. Set up environment variables
echo "‚öôÔ∏è  Step 5: Setting up environment..."
ENV_FILE=".env"

cat > $ENV_FILE << EOF
# Local Voice Pipeline Configuration
# Generated by setup script on $(date)

# Ollama LLM Configuration
OLLAMA_MODEL="$MODEL"
OLLAMA_BASE_URL="http://localhost:11434/v1"

# Whisper STT Configuration
CUDA_AVAILABLE="true"

# Kokoro TTS Configuration (existing)
KOKORO_VOICE_ID="af_bella"
KOKORO_SAMPLE_RATE="24000"

# Optional: If you have custom model paths
# KOKORO_MODEL_PATH="/path/to/kokoro/model_fp16.onnx"
# KOKORO_VOICES_PATH="/path/to/kokoro/voices-v1.0.bin"
EOF

echo "‚úÖ Environment configuration saved to $ENV_FILE"

# 6. Test Ollama connection
echo "üß™ Step 6: Testing Ollama connection..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚úÖ Ollama API is responding"
    
    # Test model inference
    echo "üîç Testing model inference..."
    TEST_RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"$MODEL\",\"prompt\":\"Hello\",\"stream\":false}" | \
        python3 -c "import json,sys; print(json.load(sys.stdin).get('response','')[:50])")
    
    if [ -n "$TEST_RESPONSE" ]; then
        echo "‚úÖ Model inference test successful: $TEST_RESPONSE..."
    else
        echo "‚ö†Ô∏è  Model inference test failed - but setup continues"
    fi
else
    echo "‚ùå Ollama API not responding - please check Ollama installation"
    exit 1
fi

# 7. Check GPU availability (for Whisper)
echo "üîç Step 7: Checking GPU availability..."
if command -v nvidia-smi &> /dev/null; then
    if nvidia-smi &> /dev/null; then
        echo "‚úÖ NVIDIA GPU detected - Whisper will use CUDA acceleration"
    else
        echo "‚ö†Ô∏è  NVIDIA GPU detected but driver issues - Whisper will use CPU"
        sed -i 's/CUDA_AVAILABLE="true"/CUDA_AVAILABLE="false"/' $ENV_FILE
    fi
else
    echo "‚ÑπÔ∏è  No NVIDIA GPU detected - Whisper will use CPU (slower but works)"
    sed -i 's/CUDA_AVAILABLE="true"/CUDA_AVAILABLE="false"/' $ENV_FILE
fi

# 8. Display completion message
echo ""
echo "üéâ LOCAL VOICE PIPELINE SETUP COMPLETE!"
echo "======================================"
echo ""
echo "üìã Configuration Summary:"
echo "   ‚Ä¢ LLM Model: $MODEL"
echo "   ‚Ä¢ Ollama URL: http://localhost:11434/v1"
echo "   ‚Ä¢ Environment: $ENV_FILE"
echo ""
echo "üöÄ To start the pipeline:"
echo "   source $ENV_FILE  # Load environment"
echo "   python src/main.py"
echo ""
echo "üîß To change models later:"
echo "   ollama pull llama3.1:7b  # Download different model"
echo "   # Then update OLLAMA_MODEL in $ENV_FILE"
echo ""
echo "üí° Tips:"
echo "   ‚Ä¢ Use llama3.2:1b for fastest inference"
echo "   ‚Ä¢ Use llama3.2:3b for balanced performance (recommended)"
echo "   ‚Ä¢ Use llama3.1:7b for best quality"
echo ""
echo "üÜò If you encounter issues:"
echo "   ‚Ä¢ Check Ollama logs: ollama logs"
echo "   ‚Ä¢ Restart Ollama: pkill ollama && ollama serve"
echo "   ‚Ä¢ Test manually: ollama run $MODEL" 